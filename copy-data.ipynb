{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Please type in the following variables\n",
        "from datetime import datetime \n",
        "data_start_dt = datetime(2017,1,1)\n",
        "data_end_dt = datetime(2018,12,31)\n",
        "temp_local_dir = 'tmp'                  # Local dir for downloading data\n",
        "storage_account_name = \"amldbxadls\"     # ADLS account\n",
        "container_name = \"data\"                 # ADLS container, make sure it exists\n",
        "path_to_raw_data = 'nyctlcraw'          # path in ADLS container\n",
        "path_to_cleaned_date = 'nyctlccleaned'  # path in ADLS container\n",
        "datastore_name = 'adlsgen2store'        # Name of Datastore representing ADLS in Azure ML\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1679773658343
        }
      },
      "outputs": [],
      "source": [
        "# this block downloads the files into a folder named `temp_local_dir`\n",
        "# Note: if such folder exists, it will be deleted. Make sure to back up valuable data.\n",
        "\n",
        "# Do not change beyond this point\n",
        "from azureml.opendatasets import NycTlcGreen\n",
        "import os \n",
        "import shutil\n",
        "if os.path.exists(temp_local_dir):\n",
        "    # Delete the folder if it exists\n",
        "    shutil.rmtree(temp_local_dir)\n",
        "os.makedirs(temp_local_dir)\n",
        "NycTlcGreen.get_file_dataset(data_start_dt, data_end_dt, False).download(f'./{temp_local_dir}',)\n",
        "shutil.move(f'{temp_local_dir}/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc', temp_local_dir)\n",
        "shutil.rmtree(f'{temp_local_dir}/https%3A')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1679774970083
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "%pip install azure-storage-file-datalake"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1679774630456
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "# Do not change \n",
        "from azureml.core import Workspace\n",
        "from azure.storage.filedatalake import DataLakeFileClient, FileSystemClient\n",
        "from azure.identity import ClientSecretCredential\n",
        "from azure.core.exceptions import ResourceExistsError\n",
        "ws = Workspace.from_config()\n",
        "client_id = ws.get_default_keyvault().get_secret('client-id')\n",
        "tenant_id = ws.get_default_keyvault().get_secret('tenant-id')\n",
        "client_secret = ws.get_default_keyvault().get_secret('client-secret')\n",
        "endpoint = f\"https://{storage_account_name}.dfs.core.windows.net\"\n",
        "\n",
        "\n",
        "# Set the source directory path and the destination directory path in ADLS Gen2\n",
        "source_directory_path = f\"{temp_local_dir}/nyctlc/green\"\n",
        "destination_directory_path = path_to_raw_data\n",
        "\n",
        "# Create a credential object using your Azure AD application credentials\n",
        "credential = ClientSecretCredential(tenant_id, client_id, client_secret)\n",
        "\n",
        "# Create a DataLakeFileClient object for the destination directory in ADLS Gen2\n",
        "file_system_client = FileSystemClient(endpoint, file_system_name=container_name, credential=credential)\n",
        "destination_directory_client = file_system_client.get_directory_client(destination_directory_path)\n",
        "\n",
        "# Upload the local files to ADLS Gen2\n",
        "for root, directories, files in os.walk(source_directory_path):\n",
        "    for file_name in files:\n",
        "        local_file_path = os.path.join(root, file_name)\n",
        "        destination_file_path = os.path.relpath(local_file_path, source_directory_path)\n",
        "        destination_file_client = destination_directory_client.get_file_client(destination_file_path)\n",
        "        with open(local_file_path, \"rb\") as f:\n",
        "            destination_file_client.upload_data(f.read(), overwrite=True)\n",
        "        print(f\"UPLOADED: {local_file_path} \")\n",
        "\n",
        "print(\"All raw files have been uploaded to ADLS Gen2.\")\n",
        "\n",
        "file_system_client.get_directory_client(path_to_cleaned_date).create_directory()\n",
        "print(\"Created an empty directory for cleaned data in ADLS Gen2.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create and register ADLS datastore\n",
        "from azureml.core import Datastore\n",
        "ds = Datastore.register_azure_data_lake_gen2(ws, \n",
        "    datastore_name=datastore_name, \n",
        "    filesystem=container_name, \n",
        "    account_name=storage_account_name,\n",
        "    tenant_id=tenant_id,\n",
        "    client_id=client_id, \n",
        "    client_secret=client_secret, \n",
        "    grant_workspace_access=True,\n",
        "    subscription_id=ws.subscription_id,\n",
        "    resource_group=ws.resource_group,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create and register raw dataset\n",
        "from azureml.core import Dataset \n",
        "d=Dataset.Tabular.from_parquet_files((ds, path_to_raw_data))\n",
        "d.register(\n",
        "    ws, 'nyctlcraw', \n",
        "    description=f'This dataset contains the raw New York Taxi data from {data_start_dt.date()} to {data_end_dt.date()}. It was downloaded using `azureml.opendatasets`. ',\n",
        "    tags={},\n",
        "    create_new_version=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "d=Dataset.Tabular.from_parquet_files((ds,path_to_cleaned_date), validate=False)\n",
        "d.register(\n",
        "    ws, 'nyctlccleaned', \n",
        "    description=f'This dataset contains the cleaned New York Taxi data based on `nyctlcraw` from {data_start_dt.date()} to {data_end_dt.date()}. It was downloaded using `azureml.opendatasets`. ',\n",
        "    tags={},\n",
        "    create_new_version=True\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3.8 - AzureML",
      "language": "python",
      "name": "python38-azureml"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
